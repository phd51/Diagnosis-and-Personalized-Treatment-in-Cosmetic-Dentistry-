# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_gGSTyAtw9p1nq71DSncVLEeTzGTzBJb
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **libraries**"""

pip install Pillow

pip install rembg

!pip install onnxruntime

"""# **Phase 1-Pre-Processing of Cosmetic dentistry Images**

# **Min-Max Normalization**
"""

import cv2
import numpy as np
import os
import matplotlib.pyplot as plt
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

def minmax_normalization(image):
    """Apply Min-Max Normalization to an image"""
    image = np.array(image, dtype=np.float32)
    min_val = np.min(image)
    max_val = np.max(image)
    normalized_image = (image - min_val) / (max_val - min_val)
    return normalized_image

def process_images(input_main_folder, output_main_folder):
    """Process all images in the main folder and subfolders"""
    for root, dirs, files in os.walk(input_main_folder):
        for file in files:
            if file.endswith(('.png', '.jpg', '.jpeg','.bmp')):  # Filter image files
                input_path = os.path.join(root, file)

                # Create corresponding output folder structure
                relative_path = os.path.relpath(root, input_main_folder)
                output_folder = os.path.join(output_main_folder, relative_path)
                os.makedirs(output_folder, exist_ok=True)

                output_path = os.path.join(output_folder, file)

                # Read the image
                image = cv2.imread(input_path)

                if image is None:
                    print(f"Skipping {input_path} (not a valid image)")
                    continue

                # Apply Min-Max normalization
                normalized_image = minmax_normalization(image)

                # Save the normalized image
                cv2.imwrite(output_path, (normalized_image * 255).astype(np.uint8))  # Convert back to uint8

                # Display input and output images
                display_images(image, normalized_image)

                print(f"Processed and saved: {output_path}")

def display_images(input_image, output_image):
    """Display input and normalized images side by side"""
    input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display
    output_image_rgb = (output_image * 255).astype(np.uint8)  # Convert normalized image back to 0-255

    plt.figure(figsize=(10, 5))

    # Show input image
    plt.subplot(1, 2, 1)
    plt.imshow(input_image_rgb)
    plt.title("Input Image")
    plt.axis('off')

    # Show normalized image
    plt.subplot(1, 2, 2)
    plt.imshow(output_image_rgb)
    plt.title("Normalized Image")
    plt.axis('off')

    plt.show()

# Define main input and output folder paths
input_main_folder = ''  # Your input main folder
output_main_folder = ''  # Output folder

# Process images and display results
process_images(input_main_folder, output_main_folder)

print("All images processed successfully!")

"""# **Augumentation Technique**"""

import cv2
import numpy as np
import os
import matplotlib.pyplot as plt

def rotate_image(image, angles=[45, 90, 180]):
    rotated_images = []
    for angle in angles:
        center = (image.shape[1] // 2, image.shape[0] // 2)
        matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(image, matrix, (image.shape[1], image.shape[0]))
        rotated_images.append(rotated)
    return rotated_images

def flip_image(image):
    return cv2.flip(image, 1), cv2.flip(image, 0)  # Horizontal and Vertical flips

def scale_image(image, scale_factors=[0.8, 1.2]):
    return [cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR) for scale in scale_factors]

def adjust_brightness_contrast(image, a=1.5):
    mu = np.mean(image)
    adjusted_image = a * (image - mu) + mu
    return np.clip(adjusted_image, 0, 255).astype(np.uint8)

def display_images(original_image, augmented_images):
    plt.figure(figsize=(12, 8))

    # Display Original Image
    plt.subplot(1, len(augmented_images) + 1, 1)
    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
    plt.title("Original Image")
    plt.axis('off')

    # Display Augmented Images
    for i, img in enumerate(augmented_images):
        plt.subplot(1, len(augmented_images) + 1, i + 2)
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        plt.title(f"Aug {i+1}")
        plt.axis('off')

    plt.show()

def process_images_in_folder(input_main_folder, output_main_folder):
    for root, _, files in os.walk(input_main_folder):
        for filename in files:
            if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                input_path = os.path.join(root, filename)
                relative_path = os.path.relpath(root, input_main_folder)
                output_folder = os.path.join(output_main_folder, relative_path)

                os.makedirs(output_folder, exist_ok=True)

                image = cv2.imread(input_path)
                if image is None:
                    print(f"Skipping {input_path} (Invalid Image)")
                    continue

                rotated_images = rotate_image(image)
                flipped_h, flipped_v = flip_image(image)
                scaled_images = scale_image(image)
                adjusted_image = adjust_brightness_contrast(image)

                augmented_images = rotated_images + [flipped_h, flipped_v] + scaled_images + [adjusted_image]

                # Save augmented images
                for i, aug_img in enumerate(augmented_images):
                    output_image_path = os.path.join(output_folder, f"{os.path.splitext(filename)[0]}_aug_{i+1}.jpg")
                    cv2.imwrite(output_image_path, aug_img)

                # Display input and output images
                display_images(image, augmented_images)

                print(f"Processed: {input_path} â†’ {output_folder}")

# Example usage
input_main_folder = ''
output_main_folder = ''

process_images_in_folder(input_main_folder, output_main_folder)

"""# **Phase 2-Segmentation of Pre-processed Cosmetic dentistry Images with Proposed DeepSegNet+**"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Concatenate, AveragePooling2D
from rembg import remove
from PIL import Image
import os
import shutil
import matplotlib.pyplot as plt
from google.colab import drive
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, Input
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Model

def atrous_spatial_pyramid_pooling(x):
    """ASPP module with atrous convolutions."""
    conv_1x1 = Conv2D(256, (1, 1), padding="same", activation='relu')(x)
    conv_3x3_r6 = Conv2D(256, (3, 3), dilation_rate=6, padding="same", activation='relu')(x)
    conv_3x3_r12 = Conv2D(256, (3, 3), dilation_rate=12, padding="same", activation='relu')(x)
    conv_3x3_r18 = Conv2D(256, (3, 3), dilation_rate=18, padding="same", activation='relu')(x)

    pooling = tf.keras.layers.GlobalAveragePooling2D()(x)
    pooling = tf.keras.layers.Reshape((1, 1, x.shape[-1]))(pooling)
    pooling = Conv2D(256, (1, 1), padding="same", activation='relu')(pooling)
    pooling = UpSampling2D(size=(x.shape[1], x.shape[2]), interpolation='bilinear')(pooling)

    aspp_output = Concatenate()([conv_1x1, conv_3x3_r6, conv_3x3_r12, conv_3x3_r18, pooling])
    return Conv2D(256, (1, 1), padding="same", activation='relu')(aspp_output)

def build_model(input_shape=(224, 224, 3)):
    """Builds the deep learning model based on EfficientNet-SAM with ASPP."""
    inputs = Input(shape=input_shape)
    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=inputs)

    # Extract feature map from EfficientNet-SAM
    feature_map = base_model.get_layer("block7a_project_conv").output  # (7,7,256)

    # ASPP Module
    aspp_output = atrous_spatial_pyramid_pooling(feature_map)  # (7,7,256)

    # Gradual upsampling
    upsample1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(aspp_output)  # (14,14,256)
    upsample2 = UpSampling2D(size=(2, 2), interpolation='bilinear')(upsample1)  # (28,28,256)
    upsample3 = UpSampling2D(size=(2, 2), interpolation='bilinear')(upsample2)  # (56,56,256)  <-- Fix!

    conv1x1 = Conv2D(128, (1, 1), padding="same", activation='relu')(base_model.get_layer("block2a_project_conv").output)  # (56,56,128)

    concat = Concatenate()([upsample3, conv1x1])  # Both now (56,56,*)

    conv3x3 = Conv2D(64, (3, 3), padding="same", activation='relu')(concat)
    upsample4 = UpSampling2D(size=(4, 4), interpolation='bilinear')(conv3x3)  # (224,224,64)

    outputs = Conv2D(3, (1, 1), activation='sigmoid')(upsample4)  # Output with 3 channels (RGB)

    model = Model(inputs, outputs)
    return model
# Instantiate and compile the model
model = build_model()
model.compile(optimizer='adam', loss='mse')
model.summary()
# Define main input and output folder paths
main_input_folder = ''  # Main input folder containing subfolders
main_output_folder = ''  # Main output folder for processed images

# Ensure the main output directory exists
os.makedirs(main_output_folder, exist_ok=True)

# Process all images in the main folder and subfolders
for root, dirs, files in os.walk(main_input_folder):
    for file in files:
        if file.endswith(('.png', '.jpg', '.jpeg','.bmp')):  # Filter only image files
            input_path = os.path.join(root, file)

            # Create corresponding output folder structure
            relative_path = os.path.relpath(root, main_input_folder)
            output_folder = os.path.join(main_output_folder, relative_path)
            os.makedirs(output_folder, exist_ok=True)

            output_path = os.path.join(output_folder, file)

            # Open the image
            input_image = Image.open(input_path)

            # Remove background
            output_image = remove(input_image)

            # Check if image has an alpha channel (RGBA)
            if output_image.mode == "RGBA":
                output_image = output_image.convert("RGB")  # Convert to RGB before saving

            # Save the output image (convert to PNG if you need transparency)
            output_path = os.path.join(output_folder, file.replace('.jpg', '.png'))  # Save as PNG
            output_image.save(output_path, format="PNG")

            # Display input and output images
            fig, axes = plt.subplots(1, 2, figsize=(10, 5))

            # Show input image
            axes[0].imshow(input_image)
            axes[0].axis('off')
            axes[0].set_title("Input Image")

            # Show output image
            axes[1].imshow(output_image)
            axes[1].axis('off')
            axes[1].set_title("Output Image")

            plt.show()

            print(f"Processed and saved: {output_path}")

print("All images processed successfully!")

"""# **Feature Extraction**
# **Gabor Filter & DCT for texture.**
# **Color Histogram for color information.**
# **Facial Landmarks & Fourier Descriptors for shape.**
"""

pip install mediapipe

import cv2
import time
import mediapipe as mp
import os
from google.colab.patches import cv2_imshow  # Import this to show images in Colab

# Grabbing the Holistic Model from Mediapipe and initializing the model
mp_holistic = mp.solutions.holistic
holistic_model = mp_holistic.Holistic(
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# Initializing the drawing utils for drawing the facial landmarks on the image
mp_drawing = mp.solutions.drawing_utils

# Define the main folder path that contains multiple subfolders
main_folder_path = ''

# Function to process images in subfolders
def process_images_in_folder(main_folder_path):
    for root, dirs, files in os.walk(main_folder_path):  # Traverse the folder structure
        for file in files:
            # Check if the file is an image file
            if file.lower().endswith(('.bmp', '.jpg', '.png', '.jpeg')):
                image_path = os.path.join(root, file)
                print(f"Processing image: {image_path}")

                # Read the image from the given path
                frame = cv2.imread(image_path)

                # Resize the frame for better view
                frame = cv2.resize(frame, (800, 600))

                # Convert the image from BGR to RGB
                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # Make predictions using the holistic model
                image.flags.writeable = False
                results = holistic_model.process(image)
                image.flags.writeable = True

                # Convert back the RGB image to BGR
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

                # Drawing the Facial Landmarks
                if results.face_landmarks:
                    mp_drawing.draw_landmarks(
                        image,
                        results.face_landmarks,
                        mp_holistic.FACEMESH_CONTOURS,
                        mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=1, circle_radius=1),
                        mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=1, circle_radius=1)
                    )

                # Drawing Right Hand Landmarks
                if results.right_hand_landmarks:
                    mp_drawing.draw_landmarks(
                        image,
                        results.right_hand_landmarks,
                        mp_holistic.HAND_CONNECTIONS
                    )

                # Drawing Left Hand Landmarks
                if results.left_hand_landmarks:
                    mp_drawing.draw_landmarks(
                        image,
                        results.left_hand_landmarks,
                        mp_holistic.HAND_CONNECTIONS
                    )

                # Display the resulting image using cv2_imshow (Google Colab-specific function)
                cv2_imshow(image)

                # Wait for key press and close the window (cv2.waitKey is not needed in Colab)
                cv2.waitKey(0)
                cv2.destroyAllWindows()

# Call the function to process images in the main folder (and subfolders)
process_images_in_folder(main_folder_path)

# Code to access landmarks
for landmark in mp_holistic.HandLandmark:
    print(landmark, landmark.value)

# Print specific hand landmark, in this case, the "WRIST" landmark
print(mp_holistic.HandLandmark.WRIST.value)

import cv2
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.models import Model
import dlib
from scipy.fftpack import dct
from skimage.filters import gabor
import mediapipe as mp
from google.colab.patches import cv2_imshow  # Import this to show images in Colab

# Load the pre-trained EfficientNet model
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
feature_model = Model(inputs=base_model.input, outputs=base_model.output)

# Initialize dlib's face detector
detector = dlib.get_frontal_face_detector()

# Grabbing the Holistic Model from Mediapipe and initializing the model
mp_holistic = mp.solutions.holistic
holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# Function to extract Gabor filter features
def gabor_features(img):
    gabor_kernels = []
    for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]:  # Different orientations
        for frequency in [0.1, 0.3, 0.5]:
            real, _ = gabor(img, frequency=frequency, theta=theta)
            gabor_kernels.append(real.mean())
    return gabor_kernels

# Function to compute DCT (Discrete Cosine Transform) coefficients
def dct_features(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    dct_coeff = dct(dct(img_gray, axis=0), axis=1)
    return dct_coeff[:10, :10].flatten()  # Taking top-left 10x10 coefficients

# Function to compute color histogram (RGB channels)
def color_histogram(img):
    hist = []
    for i in range(3):  # RGB channels
        hist.extend(cv2.calcHist([img], [i], None, [16], [0, 256]).flatten())
    return hist

# Function to extract facial landmarks using MediaPipe
def extract_landmarks(image):
    results = holistic_model.process(image)
    landmarks = []
    if results.face_landmarks:
        for landmark in results.face_landmarks.landmark:
            landmarks.append([landmark.x, landmark.y, landmark.z])
    return landmarks

# Function to extract features using EfficientNet
def extract_features(img_path):
    img = cv2.imread(img_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Detect faces using dlib
    faces = detector(gray)
    if len(faces) == 0:
        return None  # No face detected

    # Take the first detected face (can handle multiple faces if needed)
    face = faces[0]
    x, y, w, h = (face.left(), face.top(), face.width(), face.height())

    # Crop and resize the face region
    face_img = img[y:y+h, x:x+w]
    face_img = cv2.resize(face_img, (224, 224))  # Resize to match EfficientNet input size

    # Preprocess image for EfficientNet
    img_array = image.img_to_array(face_img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)

    # Extract features using EfficientNet
    features = feature_model.predict(img_array)
    return features.flatten()  # Flatten the output to a 1D feature vector

# Define input and output paths
input_folder = ""  # Update with your folder
output_csv = ""

data = []

# Extract features and labels from each image in the folder
for root, dirs, files in os.walk(input_folder):
    for file in files:
        if file.endswith(('.jpg', '.png', '.jpeg', '.bmp')):
            img_path = os.path.join(root, file)
            label = os.path.basename(root)  # Folder name as label

            # Extract features
            features = extract_features(img_path)
            if features is not None:
                # Extract traditional features
                img = cv2.imread(img_path)
                gabor_feat = gabor_features(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))
                dct_feat = dct_features(img)
                hist_feat = color_histogram(img)

                # Extract facial landmarks
                landmarks = extract_landmarks(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                landmark_feat = [landmark for sublist in landmarks for landmark in sublist]  # Flatten the landmarks

                # Combine all features (EfficientNet features + traditional features)
                feature_vector = [img_path, label] + list(features) + gabor_feat + list(dct_feat) + hist_feat + landmark_feat
                data.append(feature_vector)

# Save the features and labels to a CSV file
columns = ['Image_Path', 'Label'] + [f'Feature_{i}' for i in range(len(features))] + \
          [f'Gabor_{i}' for i in range(len(gabor_feat))] + \
          [f'DCT_{i}' for i in range(len(dct_feat))] + \
          [f'Hist_{i}' for i in range(len(hist_feat))] + \
          [f'Landmark_{i}' for i in range(len(landmark_feat))]

df = pd.DataFrame(data, columns=columns)
df.to_csv(output_csv, index=False)

print(f"Feature extraction with labels completed. Data saved to {output_csv}")
df

"""# **Feature Selection**
# **Particle Swarm Optimization (PSO)and Simulated Annealing (SA)**
"""

import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models.vision_transformer import vit_b_16
# Load your dataset
data = pd.read_csv('')
# Function to evaluate the objective (fitness) function
def evaluate_fitness(N, k, data):
    # Example: use the first N features for evaluation
    selected_features = data.iloc[:, :int(N)]  # Select first N features

    # You can apply your desired feature evaluation here, such as a classifier
    # For simplicity, we will use the number of selected features as the fitness value
    fitness = N * k  # Example: product of N and k as fitness value
    return fitness

# Update velocity and position
def update_velocity_position(particle, pbest, gbest, w, c1, c2):
    # Update velocity using the PSO equation
    r1, r2 = random.random(), random.random()

    # Cognitive and social components
    v_N_new = w * particle[2] + c1 * r1 * (pbest[0] - particle[0]) + c2 * r2 * (gbest[0] - particle[0])
    v_k_new = w * particle[3] + c1 * r1 * (pbest[1] - particle[1]) + c2 * r2 * (gbest[1] - particle[1])

    # Update position for N and k (just adding the velocity to current position)
    N_new = particle[0] + v_N_new
    k_new = particle[1] + v_k_new

    # Return the new position and updated velocities
    return [N_new, k_new, v_N_new, v_k_new]

# Fire Module (from SqueezeNet)
class FireModule(nn.Module):
    def __init__(self, in_channels, squeeze_channels, expand_channels):
        super(FireModule, self).__init__()
        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)
        self.expand1x1 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=1)
        self.expand3x3 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=3, padding=1)

    def forward(self, x):
        x = F.relu(self.squeeze(x))
        return torch.cat([F.relu(self.expand1x1(x)), F.relu(self.expand3x3(x))], dim=1)

# Residual Connection and Adaptive Enhancement Processing (RC-AEP Module)
class RCAEPModule(nn.Module):
    def __init__(self, in_channels):
        super(RCAEPModule, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)

    def forward(self, x):
        residual = x
        x = F.relu(self.conv1(x))
        x = self.conv2(x)
        return F.relu(x + residual)  # Residual Connection

# Dense Block
class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            self.layers.append(nn.Sequential(
                nn.Conv2d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1),
                nn.BatchNorm2d(growth_rate),
                nn.ReLU(inplace=True)
            ))

    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], dim=1)  # Dense connection
        return x

# Transition Layer
class TransitionLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(TransitionLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.pool = nn.AvgPool2d(2, stride=2)

    def forward(self, x):
        x = F.relu(self.conv(x))
        return self.pool(x)

# Vision Transformer (ViT) Integration
class VisionTransformerModule(nn.Module):
    def __init__(self):
        super(VisionTransformerModule, self).__init__()
        self.vit = vit_b_16(pretrained=True)

    def forward(self, x):
        x = self.vit(x)
        return x

# Complete Model
class CustomModel(nn.Module):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dense1 = DenseBlock(64, 32, 3)
        self.fire = FireModule(160, 32, 64)
        self.transition = TransitionLayer(128, 64)
        self.vision_transformer = VisionTransformerModule()
        self.rcaep = RCAEPModule(64)
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = self.dense1(x)
        x = self.fire(x)
        x = self.transition(x)
        x = self.vision_transformer(x)
        x = self.rcaep(x)
        x = self.global_avg_pool(x)
        x = torch.flatten(x, start_dim=1)
        x = torch.sigmoid(self.fc(x))
        return x
# Function to evaluate the objective (fitness) function
def evaluate_fitness(N, k, data):
    # Example: use the first N features for evaluation
    selected_features = data.iloc[:, :int(N)]  # Select first N features

    # You can apply your desired feature evaluation here, such as a classifier
    # For simplicity, we will use the number of selected features as the fitness value
    fitness = N * k  # Example: product of N and k as fitness value
    return fitness

# Update velocity and position
def update_velocity_position(particle, pbest, gbest, w, c1, c2):
    # Update velocity using the PSO equation
    r1, r2 = random.random(), random.random()

    # Cognitive and social components
    v_N_new = w * particle[2] + c1 * r1 * (pbest[0] - particle[0]) + c2 * r2 * (gbest[0] - particle[0])
    v_k_new = w * particle[3] + c1 * r1 * (pbest[1] - particle[1]) + c2 * r2 * (gbest[1] - particle[1])

    # Update position for N and k (just adding the velocity to current position)
    N_new = particle[0] + v_N_new
    k_new = particle[1] + v_k_new

    # Return the new position and updated velocities
    return [N_new, k_new, v_N_new, v_k_new]

# Function for ASO algorithm
def ASO(pop_size, N_max, k_max, M_iter, data):
    # Initialize population
    particles = []
    pbest = []  # Personal best positions
    gbest = []  # Global best position

    # Initialize particles with random positions and velocities
    for i in range(pop_size):
        N_init = random.uniform(0, N_max)
        k_init = random.uniform(0, k_max)
        v_N_init = random.uniform(-1, 1)
        v_k_init = random.uniform(-1, 1)
        particles.append([N_init, k_init, v_N_init, v_k_init])
        pbest.append([N_init, k_init])  # Initially, pbest is the same as the position

    # Set global best to the best particle's position
    fitness_values = [evaluate_fitness(p[0], p[1], data) for p in particles]
    best_particle_idx = fitness_values.index(min(fitness_values))  # Minimized fitness
    gbest = [particles[best_particle_idx][0], particles[best_particle_idx][1]]

    # ASO parameters
    w = 0.5  # inertia weight
    c1 = 1.5  # cognitive coefficient
    c2 = 1.5  # social coefficient

    # Main ASO loop
    for _ in range(M_iter):
        for i, particle in enumerate(particles):
            # Update velocity and position
            particles[i] = update_velocity_position(particle, pbest[i], gbest, w, c1, c2)

            # Evaluate new fitness
            fitness = evaluate_fitness(particles[i][0], particles[i][1], data)

            # Update pbest if the new fitness is better
            if fitness < evaluate_fitness(pbest[i][0], pbest[i][1], data):
                pbest[i] = [particles[i][0], particles[i][1]]

        # Update global best
        fitness_values = [evaluate_fitness(p[0], p[1], data) for p in particles]
        best_particle_idx = fitness_values.index(min(fitness_values))
        gbest = [particles[best_particle_idx][0], particles[best_particle_idx][1]]

    # Return the global best (optimal) solution
    return gbest

# Example usage
optimal_features = ASO(pop_size=10, N_max=158, k_max=100, M_iter=100, data=data)
print("Optimal Feature Set:", optimal_features)

# Extract the selected features
N_selected = int(optimal_features[0])  # N is the number of features selected
selected_data = data.iloc[:, :N_selected]

# Save the selected features to a new CSV file
selected_data.to_csv('', index=False)
print("Selected features saved to 'selected_features.csv'")
selected_data

"""# **Classification**

# **Proposed Algorithm**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing import image
from tensorflow.keras.optimizers import Adam
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset
from torchvision import transforms, models
from PIL import Image
import numpy as np
# Fire Module (from SqueezeNet)
class FireModule(nn.Module):
    def __init__(self, in_channels, squeeze_channels, expand_channels):
        super(FireModule, self).__init__()
        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)
        self.expand1x1 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=1)
        self.expand3x3 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=3, padding=1)

    def forward(self, x):
        x = F.relu(self.squeeze(x))
        return torch.cat([F.relu(self.expand1x1(x)), F.relu(self.expand3x3(x))], dim=1)

# Residual Connection and Adaptive Enhancement Processing (RC-AEP Module)
class RCAEPModule(nn.Module):
    def __init__(self, in_channels):
        super(RCAEPModule, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)

    def forward(self, x):
        residual = x
        x = F.relu(self.conv1(x))
        x = self.conv2(x)
        return F.relu(x + residual)  # Residual Connection

# Dense Block
class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            self.layers.append(nn.Sequential(
                nn.Conv2d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1),
                nn.BatchNorm2d(growth_rate),
                nn.ReLU(inplace=True)
            ))

    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], dim=1)  # Dense connection
        return x

# Transition Layer
class TransitionLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(TransitionLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.pool = nn.AvgPool2d(2, stride=2)

    def forward(self, x):
        x = F.relu(self.conv(x))
        return self.pool(x)

# Vision Transformer (ViT) Integration
class VisionTransformerModule(nn.Module):
    def __init__(self):
        super(VisionTransformerModule, self).__init__()
        self.vit = models.vit_b_16(pretrained=True)

    def forward(self, x):
        x = self.vit(x)
        return x

# Complete Model
class CustomModel(nn.Module):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dense1 = DenseBlock(64, 32, 3)
        self.fire = FireModule(160, 32, 64)
        self.transition = TransitionLayer(128, 64)
        self.vision_transformer = VisionTransformerModule()
        self.rcaep = RCAEPModule(64)
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 1)  # Change to 1 for binary classification

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = self.dense1(x)
        x = self.fire(x)
        x = self.transition(x)
        x = self.vision_transformer(x)
        x = self.rcaep(x)
        x = self.global_avg_pool(x)
        x = torch.flatten(x, start_dim=1)
        x = torch.sigmoid(self.fc(x))
        return x
# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)


# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Accuracy: {train_acc * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

"""# **DenseNet**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.optimizers import Adam

# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Load the DenseNet121 model pre-trained on ImageNet without the top layers (we'll add our own)
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model to retain pre-trained weights
base_model.trainable = False

# Build the complete model with DenseNet
model = Sequential([
    base_model,
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""### **Mask rcnn Algorithm**"""

pip install mrcnn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt
# Mask R-CNN imports
from mrcnn import utils
from mrcnn.config import Config

from mrcnn.visualize import display_instances

# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **Alexnet Algorithm**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix

# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(227, 227)):  # AlexNet uses 227x227 image size
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Convert labels to categorical (one-hot encoding)
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

# Build the AlexNet model
model = Sequential([
    # Convolutional Layer 1
    Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(227, 227, 3)),
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    # Convolutional Layer 2
    Conv2D(256, (5, 5), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    # Convolutional Layer 3
    Conv2D(384, (3, 3), activation='relu', padding='same'),

    # Convolutional Layer 4
    Conv2D(384, (3, 3), activation='relu', padding='same'),

    # Convolutional Layer 5
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    # Flatten the data
    Flatten(),

    # Fully Connected Layer 1
    Dense(4096, activation='relu'),
    Dropout(0.5),

    # Fully Connected Layer 2
    Dense(4096, activation='relu'),
    Dropout(0.5),

    # Output Layer
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample
y_test_labels = np.argmax(y_test, axis=1)  # Convert the one-hot encoded y_test back to labels

# Calculate confusion matrix
cm = confusion_matrix(y_test_labels, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test_labels, y_pred, average='weighted')
sensitivity = recall_score(y_test_labels, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test_labels, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test_labels, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **YOLO Algorithm**"""

import pandas as pd
import os
import shutil

# Load CSV file
csv_path = ""
data = pd.read_csv(csv_path)

# YOLO dataset directories
dataset_path = "/content/drive/MyDrive/Colab Notebooks/yep abnormal/segoutput_images1"
images_path = os.path.join(dataset_path, "images")
labels_path = os.path.join(dataset_path, "labels")

os.makedirs(images_path, exist_ok=True)
os.makedirs(labels_path, exist_ok=True)

# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **YOLOv3**"""

import pandas as pd
import os
import shutil
# YOLO dataset directories
dataset_path = ""
images_path = os.path.join(dataset_path, "images")
labels_path = os.path.join(dataset_path, "labels")

os.makedirs(images_path, exist_ok=True)
os.makedirs(labels_path, exist_ok=True)
# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **cnn with lstm**"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef

# Load dataset
data = pd.read_csv('')

# Specify label column
label_column_name = 'Label'
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding
y = LabelEncoder().fit_transform(y)

def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array /= 255.0  # Normalize
    return img_array

# Load images
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = X_images.reshape(-1, 224, 224, 3)  # Reshape

# Reshape for LSTM compatibility (if using LSTM model)
X_images_lstm = X_images.reshape(X_images.shape[0], 1, 224, 224, 3)  # Assuming sequence length = 1

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)
X_train_lstm, X_test_lstm, _, _ = train_test_split(X_images_lstm, y, test_size=0.2, random_state=42)

# CNN-only Model Architecture (for single image classification)
cnn_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# CNN + LSTM Model Architecture (for sequence classification, if needed)
lstm_model = Sequential([
    TimeDistributed(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3))),
    TimeDistributed(MaxPooling2D((2, 2))),
    TimeDistributed(Conv2D(64, (3, 3), activation='relu')),
    TimeDistributed(MaxPooling2D((2, 2))),
    TimeDistributed(Flatten()),
    LSTM(64, return_sequences=False),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')
])

# Choose model to use (uncomment the one you want to use)
model = cnn_model  # For CNN-only model
# model = lstm_model  # For CNN + LSTM model

# Compile model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Predictions
y_pred = np.argmax(model.predict(X_test), axis=1)

# Metrics
cm = confusion_matrix(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)

# Initialize empty lists to store metrics for each class
sensitivity_list = []
specificity_list = []
fpr_list = []
fnr_list = []
npv_list = []

# Calculate metrics per class
for i in range(cm.shape[0]):
    TN = np.sum(cm) - np.sum(cm[i, :]) - np.sum(cm[:, i]) + cm[i, i]
    FP = np.sum(cm[:, i]) - cm[i, i]
    FN = np.sum(cm[i, :]) - cm[i, i]
    TP = cm[i, i]

    sensitivity = TP / (TP + FN)  # Sensitivity (Recall)
    specificity = TN / (TN + FP)  # Specificity
    fpr = FP / (FP + TN)          # False Positive Rate
    fnr = FN / (FN + TP)          # False Negative Rate
    npv = TN / (TN + FN)          # Negative Predictive Value

    sensitivity_list.append(sensitivity)
    specificity_list.append(specificity)
    fpr_list.append(fpr)
    fnr_list.append(fnr)
    npv_list.append(npv)

# Print metrics for each class
for i in range(len(sensitivity_list)):
    print(f"Class {i + 1}:")
    print(f"  Sensitivity: {sensitivity_list[i]:.2f}")
    print(f"  Specificity: {specificity_list[i]:.2f}")
    print(f"  FPR: {fpr_list[i]:.2f}")
    print(f"  FNR: {fnr_list[i]:.2f}")
    print(f"  NPV: {npv_list[i]:.2f}")
    print()

# Overall metrics (using average values across all classes)
avg_sensitivity = np.mean(sensitivity_list)
avg_specificity = np.mean(specificity_list)
avg_fpr = np.mean(fpr_list)
avg_fnr = np.mean(fnr_list)
avg_npv = np.mean(npv_list)

print(f"Overall Average Sensitivity: {avg_sensitivity:.2f}")
print(f"Overall Average Specificity: {avg_specificity:.2f}")
print(f"Overall Average FPR: {avg_fpr:.2f}")
print(f"Overall Average FNR: {avg_fnr:.2f}")
print(f"Overall Average NPV: {avg_npv:.2f}")

# Print other metrics
print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")

"""# **comparison Graph**"""

import numpy as np
import matplotlib.pyplot as plt

# Data for each model (rows: models, columns: metrics)
models = ['Proposed', 'DenseNet[17]', 'YOLO[19]', 'MASK RCNN[20]', 'AlexNet[21]', 'YOLOv3[22]', 'Deep CNN with LSTM[23]']
metrics = ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-Score', 'MCC', 'NPV', 'FPR', 'FNR']

# Values for each metric for each model (from the table you provided)
values = {
    'Accuracy': [0.9897, 0.95811, 0.95134, 0.93545, 0.95678, 0.96215, 0.95032],
    'Precision': [0.99257, 0.93743, 0.93512, 0.93453, 0.93421, 0.94932, 0.94489],
    'Sensitivity': [0.97962, 0.94692, 0.93225, 0.92614, 0.94872, 0.95612, 0.95198],
    'Specificity': [0.98953, 0.95953, 0.94559, 0.93467, 0.96514, 0.96588, 0.95289],
    'F1-Score': [0.98552, 0.94693, 0.93676, 0.93032, 0.94283, 0.9551, 0.9506],
    'MCC': [0.97974, 0.93712, 0.93028, 0.91657, 0.92685, 0.94087, 0.93929],
    'NPV': [0.97083, 0.95452, 0.95184, 0.93101, 0.94045, 0.9523, 0.94847],
    'FPR': [0.0169, 0.04398, 0.05854, 0.05542, 0.05145, 0.0483, 0.04953],
    'FNR': [0.00904, 0.05745, 0.06778, 0.07265, 0.04821, 0.04315, 0.03892],
}

# Create radar charts for each metric
for metric in metrics:
    values_list = values[metric]

    # Number of models
    num_models = len(models)

    # Compute angle for each axis (360 degrees divided by the number of models)
    angles = np.linspace(0, 2 * np.pi, num_models, endpoint=False).tolist()
    values_list += values_list[:1]  # Ensure the graph is circular by closing it
    angles += angles[:1]  # Ensure the graph is circular by closing it

    # Plot radar graph for the current metric
    plt.figure(figsize=(6, 6))
    ax = plt.subplot(111, polar=True)
    ax.fill(angles, values_list, color='blue', alpha=0.25)
    ax.plot(angles, values_list, color='blue', linewidth=2)

    # Set the labels
    ax.set_yticklabels([])  # Remove radial ticks
    ax.set_xticks(angles[:-1])  # Remove last duplicate angle
    ax.set_xticklabels(models, fontsize=10)

    # Set title
    plt.title(f'Radar Chart for {metric}', size=14, color='blue', pad=20)

    # Display the chart
    plt.show()

"""# **Dataset:2**"""

import cv2
import numpy as np
import os
import matplotlib.pyplot as plt
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

def minmax_normalization(image):
    """Apply Min-Max Normalization to an image"""
    image = np.array(image, dtype=np.float32)
    min_val = np.min(image)
    max_val = np.max(image)
    normalized_image = (image - min_val) / (max_val - min_val)
    return normalized_image

def process_images(input_main_folder, output_main_folder):
    """Process all images in the main folder and subfolders"""
    for root, dirs, files in os.walk(input_main_folder):
        for file in files:
            if file.endswith(('.png', '.jpg', '.jpeg','.bmp')):  # Filter image files
                input_path = os.path.join(root, file)

                # Create corresponding output folder structure
                relative_path = os.path.relpath(root, input_main_folder)
                output_folder = os.path.join(output_main_folder, relative_path)
                os.makedirs(output_folder, exist_ok=True)

                output_path = os.path.join(output_folder, file)

                # Read the image
                image = cv2.imread(input_path)

                if image is None:
                    print(f"Skipping {input_path} (not a valid image)")
                    continue

                # Apply Min-Max normalization
                normalized_image = minmax_normalization(image)

                # Save the normalized image
                cv2.imwrite(output_path, (normalized_image * 255).astype(np.uint8))  # Convert back to uint8

                # Display input and output images
                display_images(image, normalized_image)

                print(f"Processed and saved: {output_path}")

def display_images(input_image, output_image):
    """Display input and normalized images side by side"""
    input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display
    output_image_rgb = (output_image * 255).astype(np.uint8)  # Convert normalized image back to 0-255

    plt.figure(figsize=(10, 5))

    # Show input image
    plt.subplot(1, 2, 1)
    plt.imshow(input_image_rgb)
    plt.title("Input Image")
    plt.axis('off')

    # Show normalized image
    plt.subplot(1, 2, 2)
    plt.imshow(output_image_rgb)
    plt.title("Normalized Image")
    plt.axis('off')

    plt.show()

# Define main input and output folder paths
input_main_folder = ''  # Your input main folder
output_main_folder = ''  # Output folder

# Process images and display results
process_images(input_main_folder, output_main_folder)

print("All images processed successfully!")

"""# **Augumentation**"""

import cv2
import numpy as np
import os
import matplotlib.pyplot as plt

def rotate_image(image, angles=[45, 90, 180]):
    rotated_images = []
    for angle in angles:
        center = (image.shape[1] // 2, image.shape[0] // 2)
        matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(image, matrix, (image.shape[1], image.shape[0]))
        rotated_images.append(rotated)
    return rotated_images

def flip_image(image):
    return cv2.flip(image, 1), cv2.flip(image, 0)  # Horizontal and Vertical flips

def scale_image(image, scale_factors=[0.8, 1.2]):
    return [cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR) for scale in scale_factors]

def adjust_brightness_contrast(image, a=1.5):
    mu = np.mean(image)
    adjusted_image = a * (image - mu) + mu
    return np.clip(adjusted_image, 0, 255).astype(np.uint8)

def display_images(original_image, augmented_images):
    plt.figure(figsize=(12, 8))

    # Display Original Image
    plt.subplot(1, len(augmented_images) + 1, 1)
    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
    plt.title("Original Image")
    plt.axis('off')

    # Display Augmented Images
    for i, img in enumerate(augmented_images):
        plt.subplot(1, len(augmented_images) + 1, i + 2)
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        plt.title(f"Aug {i+1}")
        plt.axis('off')

    plt.show()

def process_images_in_folder(input_main_folder, output_main_folder):
    for root, _, files in os.walk(input_main_folder):
        for filename in files:
            if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                input_path = os.path.join(root, filename)
                relative_path = os.path.relpath(root, input_main_folder)
                output_folder = os.path.join(output_main_folder, relative_path)

                os.makedirs(output_folder, exist_ok=True)

                image = cv2.imread(input_path)
                if image is None:
                    print(f"Skipping {input_path} (Invalid Image)")
                    continue

                rotated_images = rotate_image(image)
                flipped_h, flipped_v = flip_image(image)
                scaled_images = scale_image(image)
                adjusted_image = adjust_brightness_contrast(image)

                augmented_images = rotated_images + [flipped_h, flipped_v] + scaled_images + [adjusted_image]

                # Save augmented images
                for i, aug_img in enumerate(augmented_images):
                    output_image_path = os.path.join(output_folder, f"{os.path.splitext(filename)[0]}_aug_{i+1}.jpg")
                    cv2.imwrite(output_image_path, aug_img)

                # Display input and output images
                display_images(image, augmented_images)

                print(f"Processed: {input_path} â†’ {output_folder}")

# Example usage
input_main_folder = ''
output_main_folder = ''

process_images_in_folder(input_main_folder, output_main_folder)

"""# **segmentation**"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Concatenate, AveragePooling2D
from rembg import remove
from PIL import Image
import os
import shutil
import matplotlib.pyplot as plt
from google.colab import drive
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, Input
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Model

def atrous_spatial_pyramid_pooling(x):
    """ASPP module with atrous convolutions."""
    conv_1x1 = Conv2D(256, (1, 1), padding="same", activation='relu')(x)
    conv_3x3_r6 = Conv2D(256, (3, 3), dilation_rate=6, padding="same", activation='relu')(x)
    conv_3x3_r12 = Conv2D(256, (3, 3), dilation_rate=12, padding="same", activation='relu')(x)
    conv_3x3_r18 = Conv2D(256, (3, 3), dilation_rate=18, padding="same", activation='relu')(x)

    pooling = tf.keras.layers.GlobalAveragePooling2D()(x)
    pooling = tf.keras.layers.Reshape((1, 1, x.shape[-1]))(pooling)
    pooling = Conv2D(256, (1, 1), padding="same", activation='relu')(pooling)
    pooling = UpSampling2D(size=(x.shape[1], x.shape[2]), interpolation='bilinear')(pooling)

    aspp_output = Concatenate()([conv_1x1, conv_3x3_r6, conv_3x3_r12, conv_3x3_r18, pooling])
    return Conv2D(256, (1, 1), padding="same", activation='relu')(aspp_output)

def build_model(input_shape=(224, 224, 3)):
    """Builds the deep learning model based on EfficientNet-SAM with ASPP."""
    inputs = Input(shape=input_shape)
    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=inputs)

    # Extract feature map from EfficientNet-SAM
    feature_map = base_model.get_layer("block7a_project_conv").output  # (7,7,256)

    # ASPP Module
    aspp_output = atrous_spatial_pyramid_pooling(feature_map)  # (7,7,256)

    # Gradual upsampling
    upsample1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(aspp_output)  # (14,14,256)
    upsample2 = UpSampling2D(size=(2, 2), interpolation='bilinear')(upsample1)  # (28,28,256)
    upsample3 = UpSampling2D(size=(2, 2), interpolation='bilinear')(upsample2)  # (56,56,256)  <-- Fix!

    conv1x1 = Conv2D(128, (1, 1), padding="same", activation='relu')(base_model.get_layer("block2a_project_conv").output)  # (56,56,128)

    concat = Concatenate()([upsample3, conv1x1])  # Both now (56,56,*)

    conv3x3 = Conv2D(64, (3, 3), padding="same", activation='relu')(concat)
    upsample4 = UpSampling2D(size=(4, 4), interpolation='bilinear')(conv3x3)  # (224,224,64)

    outputs = Conv2D(3, (1, 1), activation='sigmoid')(upsample4)  # Output with 3 channels (RGB)

    model = Model(inputs, outputs)
    return model
# Instantiate and compile the model
model = build_model()
model.compile(optimizer='adam', loss='mse')
model.summary()
# Define main input and output folder paths
main_input_folder = ''  # Main input folder containing subfolders
main_output_folder = ''  # Main output folder for processed images

# Ensure the main output directory exists
os.makedirs(main_output_folder, exist_ok=True)

# Process all images in the main folder and subfolders
for root, dirs, files in os.walk(main_input_folder):
    for file in files:
        if file.endswith(('.png', '.jpg', '.jpeg','.bmp')):  # Filter only image files
            input_path = os.path.join(root, file)

            # Create corresponding output folder structure
            relative_path = os.path.relpath(root, main_input_folder)
            output_folder = os.path.join(main_output_folder, relative_path)
            os.makedirs(output_folder, exist_ok=True)

            output_path = os.path.join(output_folder, file)

            # Open the image
            input_image = Image.open(input_path)

            # Remove background
            output_image = remove(input_image)

            # Check if image has an alpha channel (RGBA)
            if output_image.mode == "RGBA":
                output_image = output_image.convert("RGB")  # Convert to RGB before saving

            # Save the output image (convert to PNG if you need transparency)
            output_path = os.path.join(output_folder, file.replace('.jpg', '.png'))  # Save as PNG
            output_image.save(output_path, format="PNG")

            # Display input and output images
            fig, axes = plt.subplots(1, 2, figsize=(10, 5))

            # Show input image
            axes[0].imshow(input_image)
            axes[0].axis('off')
            axes[0].set_title("Input Image")

            # Show output image
            axes[1].imshow(output_image)
            axes[1].axis('off')
            axes[1].set_title("Output Image")

            plt.show()

            print(f"Processed and saved: {output_path}")

print("All images processed successfully!")

"""# **Feature Extraction**"""

import cv2
import time
import mediapipe as mp
import os
from google.colab.patches import cv2_imshow  # Import this to show images in Colab

# Grabbing the Holistic Model from Mediapipe and initializing the model
mp_holistic = mp.solutions.holistic
holistic_model = mp_holistic.Holistic(
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# Initializing the drawing utils for drawing the facial landmarks on the image
mp_drawing = mp.solutions.drawing_utils

# Define the main folder path that contains multiple subfolders
main_folder_path = ''

# Function to process images in subfolders
def process_images_in_folder(main_folder_path):
    for root, dirs, files in os.walk(main_folder_path):  # Traverse the folder structure
        for file in files:
            # Check if the file is an image file
            if file.lower().endswith(('.bmp', '.jpg', '.png', '.jpeg')):
                image_path = os.path.join(root, file)
                print(f"Processing image: {image_path}")

                # Read the image from the given path
                frame = cv2.imread(image_path)

                # Resize the frame for better view
                frame = cv2.resize(frame, (800, 600))

                # Convert the image from BGR to RGB
                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # Make predictions using the holistic model
                image.flags.writeable = False
                results = holistic_model.process(image)
                image.flags.writeable = True

                # Convert back the RGB image to BGR
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

                # Drawing the Facial Landmarks
                if results.face_landmarks:
                    mp_drawing.draw_landmarks(
                        image,
                        results.face_landmarks,
                        mp_holistic.FACEMESH_CONTOURS,
                        mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=1, circle_radius=1),
                        mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=1, circle_radius=1)
                    )

                # Drawing Right Hand Landmarks
                if results.right_hand_landmarks:
                    mp_drawing.draw_landmarks(
                        image,
                        results.right_hand_landmarks,
                        mp_holistic.HAND_CONNECTIONS
                    )

                # Drawing Left Hand Landmarks
                if results.left_hand_landmarks:
                    mp_drawing.draw_landmarks(
                        image,
                        results.left_hand_landmarks,
                        mp_holistic.HAND_CONNECTIONS
                    )

                # Display the resulting image using cv2_imshow (Google Colab-specific function)
                cv2_imshow(image)

                # Wait for key press and close the window (cv2.waitKey is not needed in Colab)
                cv2.waitKey(0)
                cv2.destroyAllWindows()

# Call the function to process images in the main folder (and subfolders)
process_images_in_folder(main_folder_path)

# Code to access landmarks
for landmark in mp_holistic.HandLandmark:
    print(landmark, landmark.value)

# Print specific hand landmark, in this case, the "WRIST" landmark
print(mp_holistic.HandLandmark.WRIST.value)

import cv2
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.models import Model
import dlib
from scipy.fftpack import dct
from skimage.filters import gabor
import mediapipe as mp
from google.colab.patches import cv2_imshow  # Import this to show images in Colab

# Load the pre-trained EfficientNet model
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
feature_model = Model(inputs=base_model.input, outputs=base_model.output)

# Initialize dlib's face detector
detector = dlib.get_frontal_face_detector()

# Grabbing the Holistic Model from Mediapipe and initializing the model
mp_holistic = mp.solutions.holistic
holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# Function to extract Gabor filter features
def gabor_features(img):
    gabor_kernels = []
    for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]:  # Different orientations
        for frequency in [0.1, 0.3, 0.5]:
            real, _ = gabor(img, frequency=frequency, theta=theta)
            gabor_kernels.append(real.mean())
    return gabor_kernels

# Function to compute DCT (Discrete Cosine Transform) coefficients
def dct_features(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    dct_coeff = dct(dct(img_gray, axis=0), axis=1)
    return dct_coeff[:10, :10].flatten()  # Taking top-left 10x10 coefficients

# Function to compute color histogram (RGB channels)
def color_histogram(img):
    hist = []
    for i in range(3):  # RGB channels
        hist.extend(cv2.calcHist([img], [i], None, [16], [0, 256]).flatten())
    return hist

# Function to extract facial landmarks using MediaPipe
def extract_landmarks(image):
    results = holistic_model.process(image)
    landmarks = []
    if results.face_landmarks:
        for landmark in results.face_landmarks.landmark:
            landmarks.append([landmark.x, landmark.y, landmark.z])
    return landmarks

# Function to extract features using EfficientNet
def extract_features(img_path):
    img = cv2.imread(img_path)

    # Check if the image is loaded successfully
    if img is None:
        print(f"Error: Image at {img_path} could not be loaded.")
        return None

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Detect faces using dlib
    faces = detector(gray)
    if len(faces) == 0:
        print(f"No face detected in {img_path}.")
        return None  # No face detected

    # Take the first detected face (can handle multiple faces if needed)
    face = faces[0]
    x, y, w, h = (face.left(), face.top(), face.width(), face.height())

    # Crop and resize the face region
    face_img = img[y:y+h, x:x+w]

    # Ensure that the face region is not empty
    if face_img.size == 0:
        print(f"Empty face region in {img_path}. Skipping.")
        return None

    face_img = cv2.resize(face_img, (224, 224))  # Resize to match EfficientNet input size

    # Preprocess image for EfficientNet
    img_array = image.img_to_array(face_img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)

    # Extract features using EfficientNet
    features = feature_model.predict(img_array)
    return features.flatten()  # Flatten the output to a 1D feature vector


# Define input and output paths
input_folder = ""  # Update with your folder
output_csv = ""

data = []

# Extract features and labels from each image in the folder
for root, dirs, files in os.walk(input_folder):
    for file in files:
        if file.endswith(('.jpg', '.png', '.jpeg', '.bmp')):
            img_path = os.path.join(root, file)
            label = os.path.basename(root)  # Folder name as label

            # Extract features
            features = extract_features(img_path)
            if features is not None:
                # Extract traditional features
                img = cv2.imread(img_path)
                gabor_feat = gabor_features(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))
                dct_feat = dct_features(img)
                hist_feat = color_histogram(img)

                # Extract facial landmarks
                landmarks = extract_landmarks(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                landmark_feat = [landmark for sublist in landmarks for landmark in sublist]  # Flatten the landmarks

                # Combine all features (EfficientNet features + traditional features)
                feature_vector = [img_path, label] + list(features) + gabor_feat + list(dct_feat) + hist_feat + landmark_feat
                data.append(feature_vector)

# Save the features and labels to a CSV file
columns = ['Image_Path', 'Label'] + [f'Feature_{i}' for i in range(len(features))] + \
          [f'Gabor_{i}' for i in range(len(gabor_feat))] + \
          [f'DCT_{i}' for i in range(len(dct_feat))] + \
          [f'Hist_{i}' for i in range(len(hist_feat))] + \
          [f'Landmark_{i}' for i in range(len(landmark_feat))]

df = pd.DataFrame(data, columns=columns)
df.to_csv(output_csv, index=False)

print(f"Feature extraction with labels completed. Data saved to {output_csv}")

"""# **Feature Selection**"""

import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models.vision_transformer import vit_b_16
# Load your dataset
data = pd.read_csv('')
# Function to evaluate the objective (fitness) function
def evaluate_fitness(N, k, data):
    # Example: use the first N features for evaluation
    selected_features = data.iloc[:, :int(N)]  # Select first N features

    # You can apply your desired feature evaluation here, such as a classifier
    # For simplicity, we will use the number of selected features as the fitness value
    fitness = N * k  # Example: product of N and k as fitness value
    return fitness

# Update velocity and position
def update_velocity_position(particle, pbest, gbest, w, c1, c2):
    # Update velocity using the PSO equation
    r1, r2 = random.random(), random.random()

    # Cognitive and social components
    v_N_new = w * particle[2] + c1 * r1 * (pbest[0] - particle[0]) + c2 * r2 * (gbest[0] - particle[0])
    v_k_new = w * particle[3] + c1 * r1 * (pbest[1] - particle[1]) + c2 * r2 * (gbest[1] - particle[1])

    # Update position for N and k (just adding the velocity to current position)
    N_new = particle[0] + v_N_new
    k_new = particle[1] + v_k_new

    # Return the new position and updated velocities
    return [N_new, k_new, v_N_new, v_k_new]

# Fire Module (from SqueezeNet)
class FireModule(nn.Module):
    def __init__(self, in_channels, squeeze_channels, expand_channels):
        super(FireModule, self).__init__()
        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)
        self.expand1x1 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=1)
        self.expand3x3 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=3, padding=1)

    def forward(self, x):
        x = F.relu(self.squeeze(x))
        return torch.cat([F.relu(self.expand1x1(x)), F.relu(self.expand3x3(x))], dim=1)

# Residual Connection and Adaptive Enhancement Processing (RC-AEP Module)
class RCAEPModule(nn.Module):
    def __init__(self, in_channels):
        super(RCAEPModule, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)

    def forward(self, x):
        residual = x
        x = F.relu(self.conv1(x))
        x = self.conv2(x)
        return F.relu(x + residual)  # Residual Connection

# Dense Block
class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            self.layers.append(nn.Sequential(
                nn.Conv2d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1),
                nn.BatchNorm2d(growth_rate),
                nn.ReLU(inplace=True)
            ))

    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], dim=1)  # Dense connection
        return x

# Transition Layer
class TransitionLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(TransitionLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.pool = nn.AvgPool2d(2, stride=2)

    def forward(self, x):
        x = F.relu(self.conv(x))
        return self.pool(x)

# Vision Transformer (ViT) Integration
class VisionTransformerModule(nn.Module):
    def __init__(self):
        super(VisionTransformerModule, self).__init__()
        self.vit = vit_b_16(pretrained=True)

    def forward(self, x):
        x = self.vit(x)
        return x

# Complete Model
class CustomModel(nn.Module):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dense1 = DenseBlock(64, 32, 3)
        self.fire = FireModule(160, 32, 64)
        self.transition = TransitionLayer(128, 64)
        self.vision_transformer = VisionTransformerModule()
        self.rcaep = RCAEPModule(64)
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = self.dense1(x)
        x = self.fire(x)
        x = self.transition(x)
        x = self.vision_transformer(x)
        x = self.rcaep(x)
        x = self.global_avg_pool(x)
        x = torch.flatten(x, start_dim=1)
        x = torch.sigmoid(self.fc(x))
        return x
# Function to evaluate the objective (fitness) function
def evaluate_fitness(N, k, data):
    # Example: use the first N features for evaluation
    selected_features = data.iloc[:, :int(N)]  # Select first N features

    # You can apply your desired feature evaluation here, such as a classifier
    # For simplicity, we will use the number of selected features as the fitness value
    fitness = N * k  # Example: product of N and k as fitness value
    return fitness

# Update velocity and position
def update_velocity_position(particle, pbest, gbest, w, c1, c2):
    # Update velocity using the PSO equation
    r1, r2 = random.random(), random.random()

    # Cognitive and social components
    v_N_new = w * particle[2] + c1 * r1 * (pbest[0] - particle[0]) + c2 * r2 * (gbest[0] - particle[0])
    v_k_new = w * particle[3] + c1 * r1 * (pbest[1] - particle[1]) + c2 * r2 * (gbest[1] - particle[1])

    # Update position for N and k (just adding the velocity to current position)
    N_new = particle[0] + v_N_new
    k_new = particle[1] + v_k_new

    # Return the new position and updated velocities
    return [N_new, k_new, v_N_new, v_k_new]

# Function for ASO algorithm
def ASO(pop_size, N_max, k_max, M_iter, data):
    # Initialize population
    particles = []
    pbest = []  # Personal best positions
    gbest = []  # Global best position

    # Initialize particles with random positions and velocities
    for i in range(pop_size):
        N_init = random.uniform(0, N_max)
        k_init = random.uniform(0, k_max)
        v_N_init = random.uniform(-1, 1)
        v_k_init = random.uniform(-1, 1)
        particles.append([N_init, k_init, v_N_init, v_k_init])
        pbest.append([N_init, k_init])  # Initially, pbest is the same as the position

    # Set global best to the best particle's position
    fitness_values = [evaluate_fitness(p[0], p[1], data) for p in particles]
    best_particle_idx = fitness_values.index(min(fitness_values))  # Minimized fitness
    gbest = [particles[best_particle_idx][0], particles[best_particle_idx][1]]

    # ASO parameters
    w = 0.5  # inertia weight
    c1 = 1.5  # cognitive coefficient
    c2 = 1.5  # social coefficient

    # Main ASO loop
    for _ in range(M_iter):
        for i, particle in enumerate(particles):
            # Update velocity and position
            particles[i] = update_velocity_position(particle, pbest[i], gbest, w, c1, c2)

            # Evaluate new fitness
            fitness = evaluate_fitness(particles[i][0], particles[i][1], data)

            # Update pbest if the new fitness is better
            if fitness < evaluate_fitness(pbest[i][0], pbest[i][1], data):
                pbest[i] = [particles[i][0], particles[i][1]]

        # Update global best
        fitness_values = [evaluate_fitness(p[0], p[1], data) for p in particles]
        best_particle_idx = fitness_values.index(min(fitness_values))
        gbest = [particles[best_particle_idx][0], particles[best_particle_idx][1]]

    # Return the global best (optimal) solution
    return gbest

# Example usage
optimal_features = ASO(pop_size=10, N_max=158, k_max=100, M_iter=100, data=data)
print("Optimal Feature Set:", optimal_features)

# Extract the selected features
N_selected = int(optimal_features[0])  # N is the number of features selected
selected_data = data.iloc[:, :N_selected]

# Save the selected features to a new CSV file
selected_data.to_csv('', index=False)
print("Selected features saved to 'selected_features.csv'")
selected_data

"""# **Classification**
# **Proposed Algorithm**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing import image
from tensorflow.keras.optimizers import Adam
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset
from torchvision import transforms, models
from PIL import Image
import numpy as np
# Fire Module (from SqueezeNet)
class FireModule(nn.Module):
    def __init__(self, in_channels, squeeze_channels, expand_channels):
        super(FireModule, self).__init__()
        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)
        self.expand1x1 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=1)
        self.expand3x3 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=3, padding=1)

    def forward(self, x):
        x = F.relu(self.squeeze(x))
        return torch.cat([F.relu(self.expand1x1(x)), F.relu(self.expand3x3(x))], dim=1)

# Residual Connection and Adaptive Enhancement Processing (RC-AEP Module)
class RCAEPModule(nn.Module):
    def __init__(self, in_channels):
        super(RCAEPModule, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)

    def forward(self, x):
        residual = x
        x = F.relu(self.conv1(x))
        x = self.conv2(x)
        return F.relu(x + residual)  # Residual Connection

# Dense Block
class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            self.layers.append(nn.Sequential(
                nn.Conv2d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1),
                nn.BatchNorm2d(growth_rate),
                nn.ReLU(inplace=True)
            ))

    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], dim=1)  # Dense connection
        return x

# Transition Layer
class TransitionLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(TransitionLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.pool = nn.AvgPool2d(2, stride=2)

    def forward(self, x):
        x = F.relu(self.conv(x))
        return self.pool(x)

# Vision Transformer (ViT) Integration
class VisionTransformerModule(nn.Module):
    def __init__(self):
        super(VisionTransformerModule, self).__init__()
        self.vit = models.vit_b_16(pretrained=True)

    def forward(self, x):
        x = self.vit(x)
        return x

# Complete Model
class CustomModel(nn.Module):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dense1 = DenseBlock(64, 32, 3)
        self.fire = FireModule(160, 32, 64)
        self.transition = TransitionLayer(128, 64)
        self.vision_transformer = VisionTransformerModule()
        self.rcaep = RCAEPModule(64)
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 1)  # Change to 1 for binary classification

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = self.dense1(x)
        x = self.fire(x)
        x = self.transition(x)
        x = self.vision_transformer(x)
        x = self.rcaep(x)
        x = self.global_avg_pool(x)
        x = torch.flatten(x, start_dim=1)
        x = torch.sigmoid(self.fc(x))
        return x
# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **Densenet**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.optimizers import Adam

# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Load the DenseNet121 model pre-trained on ImageNet without the top layers (we'll add our own)
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model to retain pre-trained weights
base_model.trainable = False

# Build the complete model with DenseNet
model = Sequential([
    base_model,
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **Maskrcnn algorithm**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt
# Mask R-CNN imports
from mrcnn import utils
from mrcnn.config import Config

from mrcnn.visualize import display_instances

# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **Alexnet Algorithm**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix

# Load the dataset
data = pd.read_csv('')

# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(227, 227)):  # AlexNet uses 227x227 image size
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Convert labels to categorical (one-hot encoding)
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

# Build the AlexNet model
model = Sequential([
    # Convolutional Layer 1
    Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(227, 227, 3)),
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    # Convolutional Layer 2
    Conv2D(256, (5, 5), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    # Convolutional Layer 3
    Conv2D(384, (3, 3), activation='relu', padding='same'),

    # Convolutional Layer 4
    Conv2D(384, (3, 3), activation='relu', padding='same'),

    # Convolutional Layer 5
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    # Flatten the data
    Flatten(),

    # Fully Connected Layer 1
    Dense(4096, activation='relu'),
    Dropout(0.5),

    # Fully Connected Layer 2
    Dense(4096, activation='relu'),
    Dropout(0.5),

    # Output Layer
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample
y_test_labels = np.argmax(y_test, axis=1)  # Convert the one-hot encoded y_test back to labels

# Calculate confusion matrix
cm = confusion_matrix(y_test_labels, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test_labels, y_pred, average='weighted')
sensitivity = recall_score(y_test_labels, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test_labels, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test_labels, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **Yolo Algorithm**"""

import pandas as pd
import os
import shutil
# YOLO dataset directories
dataset_path = ""
images_path = os.path.join(dataset_path, "images")
labels_path = os.path.join(dataset_path, "labels")

os.makedirs(images_path, exist_ok=True)
os.makedirs(labels_path, exist_ok=True)
# Load the dataset
data = pd.read_csv('')
# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **Yolov3 Algorithm**"""

import pandas as pd
import os
import shutil
# YOLO dataset directories
dataset_path = ""
images_path = os.path.join(dataset_path, "images")
labels_path = os.path.join(dataset_path, "labels")

os.makedirs(images_path, exist_ok=True)
os.makedirs(labels_path, exist_ok=True)
# Load the dataset
data = pd.read_csv('')
# Specify the label column name
label_column_name = 'Label'  # Change to your actual label column name

# Separate the features (all columns except the label) and labels
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding for labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Load and preprocess images
def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load all images into a list
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = np.vstack(X_images)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# Compile the model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and display both training and validation accuracy
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)  # Get the class with the highest probability for each test sample

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate metrics
accuracy = np.trace(cm) / np.sum(cm)
precision = precision_score(y_test, y_pred, average='weighted')
sensitivity = recall_score(y_test, y_pred, average='weighted')  # Sensitivity = Recall
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # Specificity = TN / (TN + FP)
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])  # Negative Predictive Value
fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # False Positive Rate
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])  # False Negative Rate

# Print results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")
print(f"NPV: {npv:.2f}")
print(f"FPR: {fpr:.2f}")
print(f"FNR: {fnr:.2f}")

# Final training accuracy
train_acc = history.history['accuracy'][-1]
print(f"Final Training Accuracy: {train_acc * 100:.2f}%")

"""# **CNN With Lstm Algorithm**"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef

# Load dataset
data = pd.read_csv('')

# Specify label column
label_column_name = 'Label'
X = data.drop(columns=[label_column_name, 'Image_Path']).values  # Features
y = data[label_column_name].values  # Labels
image_paths = data['Image_Path'].values  # Image paths

# Label encoding
y = LabelEncoder().fit_transform(y)

def load_and_preprocess_image(image_path, target_size=(224, 224)):
    img = image.load_img(image_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array /= 255.0  # Normalize
    return img_array

# Load images
X_images = np.array([load_and_preprocess_image(img_path) for img_path in image_paths])
X_images = X_images.reshape(-1, 224, 224, 3)  # Reshape

# Reshape for LSTM compatibility (if using LSTM model)
X_images_lstm = X_images.reshape(X_images.shape[0], 1, 224, 224, 3)  # Assuming sequence length = 1

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.2, random_state=42)
X_train_lstm, X_test_lstm, _, _ = train_test_split(X_images_lstm, y, test_size=0.2, random_state=42)

# CNN-only Model Architecture (for single image classification)
cnn_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for multiclass classification
])

# CNN + LSTM Model Architecture (for sequence classification, if needed)
lstm_model = Sequential([
    TimeDistributed(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3))),
    TimeDistributed(MaxPooling2D((2, 2))),
    TimeDistributed(Conv2D(64, (3, 3), activation='relu')),
    TimeDistributed(MaxPooling2D((2, 2))),
    TimeDistributed(Flatten()),
    LSTM(64, return_sequences=False),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')
])

# Choose model to use (uncomment the one you want to use)
model = cnn_model  # For CNN-only model
# model = lstm_model  # For CNN + LSTM model

# Compile model
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Predictions
y_pred = np.argmax(model.predict(X_test), axis=1)

# Metrics
cm = confusion_matrix(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
mcc = matthews_corrcoef(y_test, y_pred)

# Initialize empty lists to store metrics for each class
sensitivity_list = []
specificity_list = []
fpr_list = []
fnr_list = []
npv_list = []

# Calculate metrics per class
for i in range(cm.shape[0]):
    TN = np.sum(cm) - np.sum(cm[i, :]) - np.sum(cm[:, i]) + cm[i, i]
    FP = np.sum(cm[:, i]) - cm[i, i]
    FN = np.sum(cm[i, :]) - cm[i, i]
    TP = cm[i, i]

    sensitivity = TP / (TP + FN)  # Sensitivity (Recall)
    specificity = TN / (TN + FP)  # Specificity
    fpr = FP / (FP + TN)          # False Positive Rate
    fnr = FN / (FN + TP)          # False Negative Rate
    npv = TN / (TN + FN)          # Negative Predictive Value

    sensitivity_list.append(sensitivity)
    specificity_list.append(specificity)
    fpr_list.append(fpr)
    fnr_list.append(fnr)
    npv_list.append(npv)

# Print metrics for each class
for i in range(len(sensitivity_list)):
    print(f"Class {i + 1}:")
    print(f"  Sensitivity: {sensitivity_list[i]:.2f}")
    print(f"  Specificity: {specificity_list[i]:.2f}")
    print(f"  FPR: {fpr_list[i]:.2f}")
    print(f"  FNR: {fnr_list[i]:.2f}")
    print(f"  NPV: {npv_list[i]:.2f}")
    print()

# Overall metrics (using average values across all classes)
avg_sensitivity = np.mean(sensitivity_list)
avg_specificity = np.mean(specificity_list)
avg_fpr = np.mean(fpr_list)
avg_fnr = np.mean(fnr_list)
avg_npv = np.mean(npv_list)

print(f"Overall Average Sensitivity: {avg_sensitivity:.2f}")
print(f"Overall Average Specificity: {avg_specificity:.2f}")
print(f"Overall Average FPR: {avg_fpr:.2f}")
print(f"Overall Average FNR: {avg_fnr:.2f}")
print(f"Overall Average NPV: {avg_npv:.2f}")

# Print other metrics
print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"F1-Score: {f1:.2f}")
print(f"MCC: {mcc:.2f}")

"""# **Comparison Graph**"""

import numpy as np
import matplotlib.pyplot as plt

# Data for each model (rows: models, columns: metrics)
models = ['Proposed', 'DenseNet[17]', 'YOLO[19]', 'MASK RCNN[20]', 'AlexNet[21]', 'YOLOv3[22]', 'Deep CNN with LSTM[23]']
metrics = ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-Score', 'MCC', 'NPV', 'FPR', 'FNR']

# Values for each metric for each model (from the table you provided)
values = {
    'Accuracy': [0.9795, 0.94673, 0.94925, 0.93111, 0.9483, 0.95549, 0.94867],
    'Precision': [0.98196, 0.92692, 0.92632, 0.92972, 0.92512, 0.93621, 0.93284],
    'Sensitivity': [0.96175, 0.93229, 0.92095, 0.91671, 0.93034, 0.94411, 0.94345],
    'Specificity': [0.97184, 0.94391, 0.93083, 0.93783, 0.95208, 0.95153, 0.94541],
    'F1-Score': [0.97391, 0.93942, 0.92623, 0.92372, 0.93853, 0.94116, 0.94599],
    'MCC': [0.96492, 0.92942, 0.92374, 0.90048, 0.91851, 0.93532, 0.92786],
    'NPV': [0.962287, 0.92489, 0.94592, 0.92258, 0.93585, 0.94014, 0.93456],
    'FPR': [0.0289, 0.05921, 0.06583, 0.06812, 0.05671, 0.04872, 0.05541],
    'FNR': [0.0105, 0.06591, 0.07472, 0.0898, 0.04873, 0.05589, 0.04345],
}

# Create radar charts for each metric
for metric in metrics:
    values_list = values[metric]

    # Number of models
    num_models = len(models)

    # Compute angle for each axis (360 degrees divided by the number of models)
    angles = np.linspace(0, 2 * np.pi, num_models, endpoint=False).tolist()
    values_list += values_list[:1]  # Ensure the graph is circular by closing it
    angles += angles[:1]  # Ensure the graph is circular by closing it

    # Plot radar graph for the current metric
    plt.figure(figsize=(6, 6))
    ax = plt.subplot(111, polar=True)
    ax.fill(angles, values_list, color='blue', alpha=0.25)
    ax.plot(angles, values_list, color='blue', linewidth=2)

    # Set the labels
    ax.set_yticklabels([])  # Remove radial ticks
    ax.set_xticks(angles[:-1])  # Remove last duplicate angle
    ax.set_xticklabels(models, fontsize=10)

    # Set title
    plt.title(f'Radar Chart for {metric}', size=14, color='blue', pad=20)

    # Display the chart
    plt.show()